
---
layout:     post
title:      每日算法学习
subtitle:   每天解决一个小问题，砥砺前行，我就是这么爱学习
date:       2019-12-04
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - 算法
---
# 每日了解一个算法题：

1、lstm的计算过程，举例说明，每一步说明各个参数的维度和变化

2、lr和svm的原理，二分类时这两个选哪种，为什么？

3、cnn的算法复杂度

4、L2 L1正则化的原理

    两者都是参数范数惩罚的正则化手段，用于权重衰减，防止过拟合
    L1：目标函数 + 惩罚系数*参数的绝对值之和，可以导出稀疏性结果，用于特征选择，机器学习中通常有很多特征，L1正则可以实现让模型只关注几个重要的特征.从推导的公式来看，权值的选择是一个max函数，其中是一个值是0，因此，大多满足条件的权值会被拉向0，达到稀疏性以及特征选择的效果。

    L2：目标函数 + 惩罚系数*参数平方和再开根号的值，可以使无助于目标函数减少的参数衰减掉，一般就是把这类参数拉向0（但只要未正则化对应的特征向量的分量不为0，该值也不会衰减为0）.实现方法是在梯度更新时，现在原来的参数值前乘以一个小于1的数（跟惩罚系数负相关），这样实现使权重尽可能小。参数小的模型更稳定，出现一点偏差，对结果的影响也不会很大，即模型的抗扰动能力就强。

    惩罚系数越大，L2生成的圆越小（约束区域越小），这个好理解，惩罚系数越大，权重值越往0靠近，此时的约束区域较小

5、adam那些优化的计算过程

6、bert和注意力的问题

7. 说一下Word2vec，看过源码吗?源码里面是如何负采样的，为什么要层次化softmax，sigmod在源码里面的计算方法是什么
    
    one-hot维度灾难，以及不能表示词汇间的语义关系   
    skipnram 和cbow ，前者用一个词预测上下文，后者用上下文预测中间词。词用one-hot向量表示，输入输出均为0-1矩阵，这样问题其实是分类问题，类别数等于词的数量
   
    训练技巧：
    1)hierarchical softmax（分层softmax）：分为两部分：1是用词向量取平均代替仿射变换，简化输入层至隐藏层的计算；2是优化隐藏层至输出层的计算，首先根据词频构造霍夫曼树，树的根节点为隐藏层词向量。树的内部节点为隐藏层的神经元（节点词向量），叶子节点为词（V维）。从根节点（词向量）->内部节点（隐藏神经元，节点词向量）->叶子节点（词上下文或中心词），这个步骤可以描述为在霍夫曼树上的一条路径，选择节点时候使用二元逻辑回归，利用sigmoid函数计算概率，概率为负走左侧，正走右侧。

    基于此，目标函数就可以是求路径最大似然，回到基于Hierarchical Softmax的word2vec本身，目标就是找到合适的所有节点的词向量和所有内部节点𝜃, 使训练样本达到最大似然。

    本质是把 N 分类问题变成 log(N)次二分类

    negative sampling 本质是预测总体类别的一个子集；同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。具体做法是除了正例再另外选择特定数量的负例，然后去更新正例和挑选的负例对应的权值和词向量。每一个训练样本做一个负采样，更新一部分权值和词向量。

8. 介绍一下实习的项目，并且问了文本相似度如何计算，项目中的矩阵分解算法ALS的原理是什么
transformer，seq2seq，svm，lda，learning to rank

9、深度学习的优化方法介绍下，SGD。Adagrad等等;Attention机制

10、信息增益和基尼指数的本质差别是什么
    
    答：这两个是决策树算法中决策树生成时进行特征选择的指标，ID3用信息增益，C4.5用信息增益比，CART用基尼系数.

    信息增益度量数据集在特征条件下经验熵减少的程度，是经验熵的绝对值减少，缺点是会优先选择取值较多的特征；

    信息增益比是信息增益除以数据集的经验熵，实质是增加了惩罚项，特征取值较多的数据集通常经验熵较大，这种方法的缺点是优先选择取值较少的特征做节点。***基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

    基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯

    特征选择的准则就是度量样本集合不确定性以及纯度的方法。本质相同，定义不同而已。但是信息增益和基尼指数不是等价的，虽然大多数时候它们的区别很小。信息增益对较混乱的集合有很好的表现力，但是基尼指数有所欠缺。另一方面，这也说明较纯的集合，尼指数可能会区分得更清楚

11、各类损失函数比较

12、各类激活函数比较

13、各类预训练模型的原理、差异、优缺点

    参见预训练那篇文章

14、基于共现矩阵分解的 GloVe

15、ngram model是什么

    基于马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词。
    假设下一个词的出现依赖它前面的一个词，则有：
    p(S)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)
    =p(w1)p(w2|w1)p(w3|w2)…p(wn|wn-1) // bigram
    假设下一个词的出现依赖它前面的两个词，则有：
    p(S)=p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)
    =p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|wn-1,wn-2) // trigram

    更大的n：对下一个词出现的约束信息更多，具有更大的辨别力；更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性。理论上，n越大越好，经验上，trigram用的最多，尽管如此，原则上，能用bigram解决，绝不使用trigram

16、Dristributed representation
    
    可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。
    词的分布式表示主要可以分为三类：
    基于矩阵的分布表示 ？
    基于聚类的分布表示 ？
    基于神经网络的分布表示 ？

17、kmeans降维带来的影响？
    
    标准的KMeans聚类算法的初始类簇质心选取都是随机形成，这种随机选择的初始类簇质心是形成随机性的根源。
    当数据的维度远远大于聚类簇的数量时，这时候的随机性非常大，如果直接聚类，一是达不到好的效果，导致KMeans聚类算法失去工程上的实用价值。二是kmeans是基于欧式距离计算距离的，此时计算量也非常大；因此，降维是有效的手段，可以显著降低欧氏距离计算量，同时提高聚类效果。
    当数据维度若小于等于聚类簇的数量时，随机性就消失了，随着某一维度就可以达到最优，此时聚类没有效果。

18、kmeans的最优K值选择？优缺点是什么？

    缺：1、欧式距离的特征无差别性。正是上述的维度无差别性，导致KMeans认为[0, 1, 0]、[0, 0, 1]、[1, 0, 0]是无差别的，从距离的角度衡量三者具有可替代性。但是考虑到具体的工程上，向量背后都是有语义或者实际意义的。KMeans这种无差别的对待数据维度，必然使得最终的聚类结果偏离业务场景的意义。




