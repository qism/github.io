---
layout:     post
title:      感知机
subtitle:
        - 概念
        - 基本形式较
        - 学习策略
        - 学习方法
        - 收敛性  
date:       2019-09-03
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - 统计学习方法
        - 机器学习
---

# 一、感知机的概念：

属于判别模型，输入特征空间，输出分类结果，通常是二分类，表示为正负和(+1,-1)，是神经网络和SVM的基础。

# 二、基本形式

假设数据集线性可分,f(x)=sign(wx+b)，模型的假设空间是所有的定义在特征空间中的线性分类模型，这个模型将特征空间以一个超平面分为两个部分。

其中，sign为符号函数，当x>=0时，sign(x)=1;当x<0时，sign(x)=-1; w为特征向量，为超平面的法向量，b为偏置，也可称为截距。

熟悉SVM的应该非常清楚这个概念的

# 三、学习策略

学习目标：寻找能够将正负样本点划分开的超平面，求出超平面解析式中的参数w和b

学习原则：损失函数最小化

定义损失函数:

1、误分类的样本数量；
2、样本到超平面的距离。感知机选择2作为损失函数

因此损失函数可表示为


```math
L(w,b) =-\tfrac{1}{||w||}\sum_{x_i\in M}y_i(wx_i+b)

---

```
其中，||w||为L2范数，通常不考虑||w||，则损失函数的解析式表示如下：
```math
L(w,b) =-\sum_{x_i\in M}y_i(wx_i+b)
```

# 四、学习方法

以损失函数最小化为目标，可以求解得到感知机模型，即w和b的值。

```math
minL(w,b) = -\sum_{x_i\in M}y_i(wx_i+b)
```

常用的感知机的学习方法是最小梯度下降法。将损失函数看成是w和b的函数，从前面的描述可知，损失函数是w和b的连续可导函数，则w和b的梯度可以表述如下：

```math
\nabla_w L(w,b)=-\sum_{x_i\in M}y_ix_i

\nabla_b L(w,b)=-\sum_{x_i\in M}y_i
```
感知机的学习算法：

输入：训练数据集M，X和Y,Y={-1,1},学习率（步长）
```math
\eta\in(0,1)
```
输出：w和b,感知机模型
```math
f(x) = sign(wx+b)
```
(1)输入初值w0,b0

(2)在训练集选择点（xi,yi）

(3)若
```math
y_i(wx_i+b)\leq 0

w_p=w_{p-1}+\eta y_ix_i

b_p=b_{p-1}+\eta y_i

```
(4)转至（2），直到训练集中没有误分类的点

直观上对这个算法进行解释，当一个实例点被误分类，位于分离超平面的错误一侧的时候，调整w和b的值，使分类的超平面向该误分类点的一侧移动，以减少该分类点于超平面的距离，直到超平面越过该分类点使其被正确分类。


# 五、感知机算法的收敛性

假设数据集线性可分，则经过有限次的迭代，一定可以找到一个超平面能够正确划分数据集，且损失函数值为0。

感知机的模型其实有很多个，从推导的过程可以发现，这取决于w和b的初始值选择以及误分类点的迭代优化顺序。若要得到唯一的超平面，那需要另外加越苏条件，即现行SVM的思想。

另外，强调一下前提条件：数据集线性可分，若不满足，则感知机算法不收敛，迭代结果会发生震荡。数据集线性可分的充要条件是正负实例点各自构成的凸壳不相交。这个定理是显而易见的。有兴趣的朋友可以动手证明一下。

感知机的对偶问题我放在SVM中介绍，这里不做详细阐述。

六、总结

感知机是线性二分类的模型，学习策略是极小化损失函数，学习算法是基于梯度下降法的损失函数最小化（最优化）算法。原始形式和对偶形式都是比较简单，且容易实现。



















































