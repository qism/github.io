---
layout:     post
title:      pytorch
subtitle:   学习笔记整理
date:       2019-11-03
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - NLP
        - Deep Learing
        - pytorch
---
# 笔记

# 矩阵操作

## 矩阵（张量）相乘

### torch.matmul（）

1、0维标量，输出标量乘积
输入为两个标量，输出标量乘积；标量乘积，返回标量
```python
M = torch.tensor([5])
w = torch.tensor([4])
print(M,w)
print(torch.matmul(M, w))
print(torch.matmul(M, w).size())

tensor([5]) tensor([4])
tensor(20)
torch.Size([])
```
-------------------------
输入为两个1-dim，输出点积(0-dim)；向量向量乘积，返回标量
```python
M = torch.rand([5])
w = torch.rand([5])

tensor([0.9838, 0.5986, 0.3991, 0.2425, 0.1890]) tensor([0.9122, 0.2223, 0.4347, 0.8645, 0.4820])
tensor(1.5048)
torch.Size([])
```
---------------------------
输入为两个2-dim,输出矩阵乘积 第一个tensor的列必须喝第二个tensor的行数相同；矩阵矩阵乘积
```python
M = torch.rand([5,2])
w = torch.rand([2,4])

torch.Size([5, 2]) torch.Size([2, 4])
tensor([[0.2861, 0.5154, 0.8948, 0.2993],
        [0.5413, 0.1947, 0.3156, 0.1045],
        [0.4629, 0.3601, 0.6115, 0.2039],
        [0.4754, 0.4319, 0.7376, 0.2461],
        [0.1595, 0.4937, 0.8629, 0.2889]])
torch.Size([5, 4])
```

-------------------------
第一个为2-dim,第二个为1-dim 输出1-dim 0.6223*0.3830+0.1783*0.4625=0.3208;矩阵向量乘积，返回的是向量
```python
M = torch.rand([2,2])
w = torch.rand([2])

tensor([[0.6223, 0.1783],
        [0.5696, 0.8494]]) tensor([0.3830, 0.4625])
tensor([0.3208, 0.6110])
torch.Size([2])
```
------------------------
第一个为1-dim，第二个为2-dim,输出1-dim；向量矩阵相乘，转为矩阵矩阵相乘，第一个向量添加1，为1*2 2*3-> 1*3 再把1删除，有点绕~~
```python
M = torch.rand([2])
w = torch.rand([2,3])

tensor([0.0095, 0.5742]) tensor([[0.3626, 0.0170, 0.3019],
        [0.9777, 0.5791, 0.0694]])
tensor([0.5649, 0.3327, 0.0427])
torch.Size([3])
```
----------------------------
如果两个自变量至少为一维且至少一个自变量为N维（其中N> 2），则返回批处理矩阵乘法。如果第一个参数是一维的，则在其维数之前添加一个1，以实现批量矩阵乘法并在其后删除。如果第二个参数为一维，则将1附加到其维上，以实现成批矩阵倍数的目的，然后将其删除。非矩阵（即批量）维度可以被广播（因此必须是可广播的）。例如，如果input为（jx1xnxm）张量，而other为（k×m×p）张量，out将是（j×k×n×p）张量。
```python
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(4)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])

batched matrix x batched matrix
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(10, 4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])

batched matrix x broadcasted matrix
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])

tensor1 = torch.randn(10, 1, 3, 4)
tensor2 = torch.randn(2, 4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 2, 3, 5])
```
---------------------------------

### torch.mm()
   矩阵相乘 (m,n) (n,p) > m * p

### torch.bmm()

   两个batch矩阵乘法,(bt,m,n) (bt,n,p) > bt*m*p

## 赋值和转换操作

### torch.transpose(inx1,idx2) 2-dim的矩阵转置

```python
batch_index = torch.randn(2, 3, 4)
batch_es = batch_index.transpose(0, 1)
batch_ee = batch_index.transpose(2, 0)
print(batch_index)
print(batch_es)
print(batch_ee)

tensor([[[-1.4525,  1.2835,  1.1216,  2.0556],
         [-1.5414, -0.3994, -1.5069,  1.2970],
         [ 1.8009,  0.5364,  1.2661,  0.9731]],
        [[ 0.3173,  0.4148,  0.3995, -0.4873],
         [-0.5183,  0.5625, -1.2070,  0.0856],
         [ 0.4155, -0.6495,  0.1983, -1.3885]]])

tensor([[[-1.4525,  1.2835,  1.1216,  2.0556],
         [ 0.3173,  0.4148,  0.3995, -0.4873]],
        [[-1.5414, -0.3994, -1.5069,  1.2970],
         [-0.5183,  0.5625, -1.2070,  0.0856]],
        [[ 1.8009,  0.5364,  1.2661,  0.9731],
         [ 0.4155, -0.6495,  0.1983, -1.3885]]])

tensor([[[-1.4525,  0.3173],
         [-1.5414, -0.5183],
         [ 1.8009,  0.4155]],
        [[ 1.2835,  0.4148],
         [-0.3994,  0.5625],
         [ 0.5364, -0.6495]],
        [[ 1.1216,  0.3995],
         [-1.5069, -1.2070],
         [ 1.2661,  0.1983]],
        [[ 2.0556, -0.4873],
         [ 1.2970,  0.0856],
         [ 0.9731, -1.3885]]])
```

### torch.permute(dims) 将tensor的维度换位
```python
s = torch.tensor([[[1,2,3],[4,5,6]]])
print(s)
print(s.permute(0,2,1))

tensor([[[1, 2, 3],
         [4, 5, 6]]])
tensor([[[1, 4],
         [2, 5],
         [3, 6]]])
```

***区别：permute可以对任意高维矩阵进行转置。
transpose只能操作2D矩阵的转置（连续使用transpose可实现permute效果）。***

### torch.type(new_type)和 torch.type_as(tensor)

type(new_type=None, async=False)如果未提供new_type，则返回类型，否则将此对象转换为指定的类型。 

type_as(tesnor)将张量转换为给定类型的张量

***区别：type()输入的为type类型，type_as输入的是其他类型的tensor(一般是已有的tensor)***

```python
self = torch.rand(3, 5)
tesnor = torch.IntTensor(2,3)
print(self)
print(self.type())
print (self.type_as(tesnor))

tensor([[0.7002, 0.9869, 0.7956, 0.5902, 0.7800],
        [0.8205, 0.2335, 0.1937, 0.7976, 0.7147],
        [0.7045, 0.7607, 0.3336, 0.3504, 0.9744]])
torch.FloatTensor
tensor([[0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0]], dtype=torch.int32)
```
## 参数处理

### torch.nn.Parameter()

在attention实现的操作中有一个用法：
```python
self.w = nn.Parameter(torch.Tensor(config.hidden_size * 2)) 
```
**value的权值是变化的，用这种方式能够将其融入模型中，使得参数可以在迭代中优化**
      
一些大佬的博客已经写得比较明白了，这里借用他们的解释：「可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。」






