---
layout:     post
title:      pytorch
subtitle:   日常学习笔记
date:       2019-11-03
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - NLP
        - Deep Learing
        - pytorch
---
# 笔记

# 矩阵（张量）相乘

## torch.matmul（）

1、0维标量，输出标量乘积
(```)输入为两个标量，输出标量乘积；标量乘积，返回标量
M = torch.tensor([5])
w = torch.tensor([4])
print(M,w)
print(torch.matmul(M, w))
print(torch.matmul(M, w).size())
##########result#########
tensor([5]) tensor([4])
tensor(20)
torch.Size([])
(```)

(```)输入为两个1-dim，输出点积(0-dim)；向量向量乘积，返回标量
M = torch.rand([5])
w = torch.rand([5])
##########result#########
tensor([0.9838, 0.5986, 0.3991, 0.2425, 0.1890]) tensor([0.9122, 0.2223, 0.4347, 0.8645, 0.4820])
tensor(1.5048)
torch.Size([])
(```)

(```)输入为两个2-dim,输出矩阵乘积 第一个tensor的列必须喝第二个tensor的行数相同；矩阵矩阵乘积
M = torch.rand([5,2])
w = torch.rand([2,4])
##########result#########
torch.Size([5, 2]) torch.Size([2, 4])
tensor([[0.2861, 0.5154, 0.8948, 0.2993],
        [0.5413, 0.1947, 0.3156, 0.1045],
        [0.4629, 0.3601, 0.6115, 0.2039],
        [0.4754, 0.4319, 0.7376, 0.2461],
        [0.1595, 0.4937, 0.8629, 0.2889]])
torch.Size([5, 4])
(```)

(```)第一个为2-dim,第二个为1-dim 输出1-dim 0.6223*0.3830+0.1783*0.4625=0.3208;矩阵向量乘积，返回的是向量
M = torch.rand([2,2])
w = torch.rand([2])
##########result#########
tensor([[0.6223, 0.1783],
        [0.5696, 0.8494]]) tensor([0.3830, 0.4625])
tensor([0.3208, 0.6110])
torch.Size([2])
(```)

(```)第一个为1-dim，第二个为2-dim,输出1-dim；向量矩阵相乘，转为矩阵矩阵相乘，第一个向量添加1，为1*2 2*3-> 1*3 再把1删除，有点绕~~
M = torch.rand([2])
w = torch.rand([2,3])
##########result#########
tensor([0.0095, 0.5742]) tensor([[0.3626, 0.0170, 0.3019],
        [0.9777, 0.5791, 0.0694]])
tensor([0.5649, 0.3327, 0.0427])
torch.Size([3])
(```)

(```)如果两个自变量至少为一维且至少一个自变量为N维（其中N> 2），则返回批处理矩阵乘法。如果第一个参数是一维的，则在其维数之前添加一个1，以实现批量矩阵乘法并在其后删除。如果第二个参数为一维，则将1附加到其维上，以实现成批矩阵倍数的目的，然后将其删除。非矩阵（即批量）维度可以被广播（因此必须是可广播的）。例如，如果input为（jx1xnxm）张量，而other为（k×m×p）张量，out将是（j×k×n×p）张量。
########################
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(4)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3])
########################
# batched matrix x batched matrix
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(10, 4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
########################
# batched matrix x broadcasted matrix
tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 3, 5])
#########################
tensor1 = torch.randn(10, 1, 3, 4)
tensor2 = torch.randn(2, 4, 5)
torch.matmul(tensor1, tensor2).size()
torch.Size([10, 2, 3, 5])
(```)

## torch.mm()
   矩阵相乘 (m,n) (n,p) > m*p

## torch.bmm()

   两个batch矩阵乘法,(bt,m,n) (bt,n,p) > bt*m*p


# 参数处理

## torch.nn.Parameter()

在attention实现的操作中有一个用法：
(```)
self.w = nn.Parameter(torch.Tensor(config.hidden_size * 2))  #value的权值是变化的，用这种方式能够将其融入模型中，使得参数可以在迭代中优化
(```)        
一些大佬的博客已经写得比较明白了，这里借用他们的解释：「可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。」

# 赋值操作








