---
layout:     post
title:      NLP预训练模型学习笔记
subtitle:   总结常用预训练模型的原理和要点
date:       2019-10-16
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - NLP
        - Deep Learing
        - 预训练
        - bert
        - XLNET
-----------------------------------------------------------

# 一、bert之前的预训练
bert之前的预训练主要是
--------------------------------------
NNLM：使用上文预测下一个单词，以上下文为条件最大化预测词的输出为目标，输入one-hot矩阵，通过不断调整映射矩阵Q，得到embedding，其目标是获取得语言模型的网络结构，词向量只是个副产物
-----------------------------------------------------------
Word2vec：

	CBOW：使用上下文预测中间词，投影层将上下文的词向量直接相加而不是拼接，同时舍弃了隐层，减少了计算量
	SKIP-gram:输入单词预测上下文

	预测阶段获得映射矩阵Q,然后下游任务，将one-hot层使用矩阵Q(预训练好的)映射到embedding，相当于用参数矩阵初始化了第一层网络参数，后续可以使用frozen或fine-tuning方式

***存在问题：静态，同一个单词使用同一参数空间，不能体现单词的不同语义***
-----------------------------------------------------------
ELMO(Embedding from Language Models)：引入上下文动态调整单词的embedding后多义词问题

    每个单词对应有word embedding、上文LSTM层embedding和下文embedding，单词最后的表征是这三个embedding的加权求和，这个权重可以在训练阶段学习

    普适性很强，在这之前有类似的工作，但是关注特定领域，以及只在NLP一些任务上的表现测试

***存在问题：1）特征抽取器选择LSTM，transformer这方面效果更好；2）ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱（elmo采用1层静态向量+2层LSTM，多层提取能力有限）***

论文：Deep contextualized word representation

关于2）在讲bert中再做对比

***上述为基于特征融合的预训练方案***
-----------------------------------------------------------
GPT(Generative Pre-Training):特征抽取器用的Transformer,这个选择很明显是很明智的；GPT的预训练仍然是以语言模型作为目标任务，但是采用的是单向的语言模型,只用了上文的信息来预测单词

***基于fine-tuning 预训练***

下游任务，首先需要将任务改为GPT的网络结构，将预训练的语言知识引入进来，然后可以用当前任务去微调（fine-tuning）网络结构，使得该网络更适合解决当前任务

***存在问题：1）没有双向特征抽取，用的是单向的，使用的是transformer的***

bert出来之后，GPT2.0仍然采用单向语言模型，但是加大了语料，改善了语料的质量，效果也有较大的提高
-----------------------------------------------------------

# 二、bert

采用和GPT相同的两阶段模型，首先，相比GPT，BERT的预训练阶段使用的是双向语言模型，预训练的语料量也比较大（bert的双向语言模型则采用encoder部分，采用了完整句子）；其次，fine-tunning，下游任务仍需要做改造

改造方式有点不同,其中对分类、句子关系和序列标注类任务改造相对简单，对于句子关系类任务，只需添加起始和终结符号，句子之间添加分隔符号，然后在输出的第一个起始符号的transformer后面添加softmax做分类；对于分类任务，类似句子关系任务，不过没有句子间的分隔符号；对于序列标注任务，输入部分与分类任务相同，即添加起始和终结符号，输出部分的每一层每个单词的transformer添加分类；

对于生成式的任务，改造有些复杂，参考「张俊林的博客」：只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。

近期的一些研究（待仔细研究）：

MASS 用bert来做seq2seq的任务
[paper](https://arxiv.org/abs/1905.02450)

UNILM 提供一种seq2seq的预训练方案，可以直接用bert模型完成，不用区分encoder和decoder  
[paper](https://arxiv.org/abs/1905.03197)

通过更改mask的方式，使得输入是双向的attention，输出是单向attention，这是符合seq2seq的输出要求的

UNILM模型，直接用单个Bert的架构做Seq2Seq，可以将它作为一种预训练手段，又或者干脆就用它来做Seq2Seq任务...


MASK掩码语言模型的两个问题：首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：
数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：
80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]
10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple
10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy. 这样做的目的是将表示偏向于实际观察到的单词。
Transformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。
其次是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。（知乎：NLP必读 | 十分钟读懂谷歌BERT模型）


与word2vec、ELMO、GPT的关系：bert的mask模型使用的是CBOW，随机选择15%的单词mask掉（实际只有这15%中的80%的词被mask，10%的词被替换成其他词，其他词不变），如果ELMO使用transformer作为特征抽取器，那就成了bert,如果GPT使用双向transformer也变成了bert；另外bert的训练语料量要大得多

创新点：在做mask双向语言模型时同时做了句子级别的预训练，这种多任务过程是一个创新

缺点：第一个预训练阶段因为采取引入[Mask]标记来Mask掉部分单词的训练模式，而Fine-tuning阶段是看不到这种被强行加入的Mask标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失；另外一个是，Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系





-----------------------------------------------------------
***在XLNet之前，几乎所有的预训练模型都是自回归的，要么从左到右，要么从右到左，尽管可以类似ELMO两个都做，然后再拼接的方式。但是跟Bert比，效果明显不足够好（这里面有RNN弱于Transformer的因素，也有双向语言模型怎么做的因素）。就如上面所说，bert存在上述缺点，那么，能不能类似Bert那样，比较充分地在自回归语言模型中，引入双向语言模型呢？因为Bert已经证明了这是非常关键的一点。这一点，想法简单，但是看上去貌似不太好做，因为从左向右的语言模型，如果我们当前根据上文，要预测某个单词Ti，那么看上去它没法看到下文的内容。具体怎么做才能让这个模型：看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？XLNet在模型方面的主要贡献其实是在这里。***

# 三 掩码语言模型 XLNet

XLNet主要的的三大部分：

1. 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(简称PLM)；这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。

2. 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；

3. 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。

对于长文本transformer的处理弱点，引入transformer xl，能够提升长文本任务的模型表现

Permutation Language Model对于生成类的NLP任务表现好于bert，主要是因为其在预训练阶段使用自回归语言模型方式



























