---
layout:     post
title:      NLP预训练模型学习笔记
subtitle:   总结常用预训练模型的原理和要点
date:       2019-10-16
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - NLP
        - Deep Learing
        - 预训练
        - bert
        - XLNET
-----------------------------------------------------------

# 一、bert之前的预训练
bert之前的预训练主要是
--------------------------------------
NNLM：使用上文预测下一个单词，以上下文为条件最大化预测词的输出为目标，输入one-hot矩阵，通过不断调整映射矩阵Q，得到embedding
-----------------------------------------------------------
Word2vec：

	CBOW：使用上下文预测中间词
	SKIP-gram:输入单词预测上下文

	预测阶段获得映射矩阵Q,然后下游任务，将one-hot层使用矩阵Q(预训练好的)映射到embedding

***存在问题：静态，同一个单词使用同一参数空间，不能体现单词的不同语义***
-----------------------------------------------------------
ELMO(Embedding from Language Models)：引入上下文动态调整单词的embedding后多义词问题

    每个单词对应有word embedding、上文LSTM层embedding和下文embedding，单词最后的表征是这三个embedding的加权求和，这个权重可以在训练阶段学习

    普适性很强，在这之前有类似的工作，但是关注特定领域，以及只在NLP一些任务上的表现测试

***存在问题：1）特征抽取器选择LSTM，transformer这方面效果更好；2）ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱***

关于2）在讲bert中再做对比？？？？？

***上述为基于特征融合的预训练方案***
-----------------------------------------------------------
GPT(Generative Pre-Training):特征抽取器用的Transformer,这个选择很明显是很明智的；GPT的预训练仍然是以语言模型作为目标任务，但是采用的是单向的语言模型,只用了上文的信息来预测单词

***基于fine-tuning 生成式 预训练？？？***

下游任务，首先需要将任务改为GPT的网络结构，将预训练的语言知识引入进来，然后可以用当前任务去微调（fine-tuning）网络结构，使得该网络更适合解决当前任务

***存在问题：1）没有双向特征抽取，用的是单向的***

bert出来之后，GPT2.0仍然采用单向语言模型，但是加大了语料，改善了语料的质量，效果也有较大的提高
-----------------------------------------------------------

# 二、bert

采用和GPT相同的两阶段模型，首先，相比GPT，BERT的预训练阶段使用的是双向语言模型，预训练的语料量也比较大；其次，fine-tunning，下游任务仍需要做改造

改造方式有点不同

优点：

缺点：第一个预训练阶段因为采取引入[Mask]标记来Mask掉部分单词的训练模式，而Fine-tuning阶段是看不到这种被强行加入的Mask标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失；另外一个是，Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系
-----------------------------------------------------------
***在XLNet之前，几乎所有的预训练模型都是自回归的，要么从左到右，要么从右到左，尽管可以类似ELMO两个都做，然后再拼接的方式。但是跟Bert比，效果明显不足够好（这里面有RNN弱于Transformer的因素，也有双向语言模型怎么做的因素）。就如上面所说，bert存在上述缺点，那么，能不能类似Bert那样，比较充分地在自回归语言模型中，引入双向语言模型呢？因为Bert已经证明了这是非常关键的一点。这一点，想法简单，但是看上去貌似不太好做，因为从左向右的语言模型，如果我们当前根据上文，要预测某个单词Ti，那么看上去它没法看到下文的内容。具体怎么做才能让这个模型：看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？XLNet在模型方面的主要贡献其实是在这里。***

# 三 掩码语言模型 XLNet

XLNet主要的的三大部分：

1. 与Bert采取De-noising Autoencoder方式不同的新的预训练目标：Permutation Language Model(简称PLM)；这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。

2. 引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；

3. 加大增加了预训练阶段使用的数据规模；Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。这个明显走的是GPT2.0的路线。

对于长文本transformer的处理弱点，引入transformer xl，能够提升长文本任务的模型表现

Permutation Language Model对于生成类的NLP任务表现好于bert，主要是因为其在预训练阶段使用自回归语言模型方式




MASS 用bert来做seq2seq的任务
[paper](https://arxiv.org/abs/1905.02450)

UNILM 提供一种seq2seq的预训练方案，可以直接用bert模型完成，不用区分encoder和decoder  这个比较难理解？要看一下原文

[paper](https://arxiv.org/abs/1905.03197)

通过更改mask的方式，使得输入是双向的attention，输出是单向attention，这是符合seq2seq的输出要求的

bert 掩码语言模型





众所周知，传统的模型预训练手段就是语言模型，比如ELMo模型就是以BiLSTM为基础架构、用两个方向的语言模型分别预训练两个方向的LSTM的，后面的OpenAI的GPT、GPT-2也是坚定不移地坚持着用祖传的（标准的、单向的）语言模型来预训练。

然而，还有更多花样的预训练玩法。

比如Bert就用了称之为“掩码语言模型（Masked Language Model）”的方式来预训练，不过这只是普通语言模型的一种变体；

还有XLNet则提出了更彻底的“Permutation Language Modeling”，我们可以称之为“乱序语言模型”；

还有UNILM模型，直接用单个Bert的架构做Seq2Seq，你可以将它作为一种预训练手段，又或者干脆就用它来做Seq2Seq任务...


























