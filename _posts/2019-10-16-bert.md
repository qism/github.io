---
layout:     post
title:      bert
subtitle:   Google 提出的当下最为流行的预训练模型
date:       2019-10-16
author:     qism
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:    
        - NLP
        - Deep Learing
        - 预训练
        - bert
---

# 一、bert


掩码语言模型








MASS 用bert来做seq2seq的任务
[paper](https://arxiv.org/abs/1905.02450)

UNILM 提供一种seq2seq的预训练方案，可以直接用bert模型完成，不用区分encoder和decoder  这个比较难理解？要看一下原文

[paper](https://arxiv.org/abs/1905.03197)

通过更改mask的方式，使得输入是双向的attention，输出是单向attention，这是符合seq2seq的输出要求的



bert 掩码语言模型





众所周知，传统的模型预训练手段就是语言模型，比如ELMo模型就是以BiLSTM为基础架构、用两个方向的语言模型分别预训练两个方向的LSTM的，后面的OpenAI的GPT、GPT-2也是坚定不移地坚持着用祖传的（标准的、单向的）语言模型来预训练。

然而，还有更多花样的预训练玩法。

比如Bert就用了称之为“掩码语言模型（Masked Language Model）”的方式来预训练，不过这只是普通语言模型的一种变体；

还有XLNet则提出了更彻底的“Permutation Language Modeling”，我们可以称之为“乱序语言模型”；

还有UNILM模型，直接用单个Bert的架构做Seq2Seq，你可以将它作为一种预训练手段，又或者干脆就用它来做Seq2Seq任务...


























